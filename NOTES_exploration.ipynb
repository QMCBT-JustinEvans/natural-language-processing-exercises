{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcbc9382",
   "metadata": {},
   "source": [
    "# NLP EDA\n",
    "\n",
    "Basically, exploration and modeling boil down to a single question:\n",
    "\n",
    "How do we quantify our data/text\n",
    "\n",
    "In this lesson, we'll explore answers to this question that will aid in visualization.\n",
    "\n",
    "- word frequency (by label)\n",
    "- ngrams\n",
    "- word cloud\n",
    "- sentiment analysis\n",
    "- other common features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc3ad2d",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Data is spam/ham text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb9aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import unicodedata\n",
    "import re\n",
    "# ------------- #\n",
    "# Local Imports #\n",
    "# ------------- #\n",
    "\n",
    "# importing sys\n",
    "import sys\n",
    "\n",
    "# adding 00_helper_files to the system path\n",
    "sys.path.insert(0, '/Users/qmcbt/codeup-data-science/00_helper_files')\n",
    "\n",
    "# env containing sensitive access credentials\n",
    "import env\n",
    "from env import user, password, host\n",
    "from env import get_db_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dace4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting basic style parameters for matplotlib\n",
    "plt.rc('figure', figsize=(13, 7))\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2868286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ADDITIONAL_STOPWORDS = [] #['r', 'u', '2', 'ltgt']\n",
    "def clean(text):\n",
    "    'A simple function to cleanup text data'\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "    text = (unicodedata.normalize('NFKD', text)\n",
    "             .encode('ascii', 'ignore')\n",
    "             .decode('utf-8', 'ignore')\n",
    "             .lower())\n",
    "    words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "    return [wnl.lemmatize(word) for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d56c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic cleaning function:\n",
    "# ADDITIONAL_STOPWORDS = ['r', 'u', '2', 'ltgt']\n",
    "\n",
    "# def clean(text):\n",
    "#     '''Simplified text cleaning function'''\n",
    "#     stopwords = nltk.corpus.stopwords.words('english') + ADDITIONAL_STOPWORDS\n",
    "#     text = text.lower()\n",
    "#     text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#     words = re.sub(r\"[^a-z0-9\\s]\", '', text)\n",
    "#     return [word for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d354b64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "id                                                         \n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acquire data from spam_db\n",
    "\n",
    "url = get_db_url(\"spam_db\")\n",
    "sql = \"SELECT * FROM spam\"\n",
    "\n",
    "df = pd.read_sql(sql, url, index_col=\"id\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d580c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb86874",
   "metadata": {},
   "source": [
    "### If we look at this in the context of a classification problem,\n",
    "we may ask:\n",
    " - What leads to a spam text?\n",
    " - What leads to a ham text?\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3708c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok lar... Joking wif u oni...'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c97afe16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this  is  a  list of strings'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall how the string method join operates -- glues together \n",
    "# all instances inside of a iterable using whatever is specified\n",
    "# ahead of the method call:\n",
    "# english: glue these words together with a empty space\n",
    "' '.join(['this ', 'is ', 'a ', 'list of strings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b211a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can do that process with a join on a Series and not just a list\n",
    "# we will do that for ham words, spam words, and all words\n",
    "# we will pass our basic cleaning on top of that\n",
    "ham_words = clean(' '.join(df[df.label == 'ham']['text']))\n",
    "spam_words = clean(' '.join(df[df.label == 'spam']['text']))\n",
    "all_words = clean(' '.join(df['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "526e23cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go',\n",
       " 'jurong',\n",
       " 'point',\n",
       " 'crazy',\n",
       " 'available',\n",
       " 'bugis',\n",
       " 'n',\n",
       " 'great',\n",
       " 'world',\n",
       " 'la',\n",
       " 'e',\n",
       " 'buffet',\n",
       " 'cine',\n",
       " 'got',\n",
       " 'amore',\n",
       " 'wat',\n",
       " 'ok',\n",
       " 'lar',\n",
       " 'joking',\n",
       " 'wif',\n",
       " 'u',\n",
       " 'oni',\n",
       " 'u',\n",
       " 'dun',\n",
       " 'say',\n",
       " 'early',\n",
       " 'hor',\n",
       " 'u',\n",
       " 'c',\n",
       " 'already',\n",
       " 'say',\n",
       " 'nah',\n",
       " 'dont',\n",
       " 'think',\n",
       " 'go',\n",
       " 'usf',\n",
       " 'life',\n",
       " 'around',\n",
       " 'though',\n",
       " 'even',\n",
       " 'brother',\n",
       " 'like',\n",
       " 'speak',\n",
       " 'treat',\n",
       " 'like',\n",
       " 'aid',\n",
       " 'patent',\n",
       " 'per',\n",
       " 'request',\n",
       " 'melle',\n",
       " 'melle',\n",
       " 'oru',\n",
       " 'minnaminunginte',\n",
       " 'nurungu',\n",
       " 'vettam',\n",
       " 'set',\n",
       " 'callertune',\n",
       " 'caller',\n",
       " 'press',\n",
       " '9',\n",
       " 'copy',\n",
       " 'friend',\n",
       " 'callertune',\n",
       " 'im',\n",
       " 'gonna',\n",
       " 'home',\n",
       " 'soon',\n",
       " 'dont',\n",
       " 'want',\n",
       " 'talk',\n",
       " 'stuff',\n",
       " 'anymore',\n",
       " 'tonight',\n",
       " 'k',\n",
       " 'ive',\n",
       " 'cried',\n",
       " 'enough',\n",
       " 'today',\n",
       " 'ive',\n",
       " 'searching',\n",
       " 'right',\n",
       " 'word',\n",
       " 'thank',\n",
       " 'breather',\n",
       " 'promise',\n",
       " 'wont',\n",
       " 'take',\n",
       " 'help',\n",
       " 'granted',\n",
       " 'fulfil',\n",
       " 'promise',\n",
       " 'wonderful',\n",
       " 'blessing',\n",
       " 'time',\n",
       " 'date',\n",
       " 'sunday',\n",
       " 'oh',\n",
       " 'kim',\n",
       " 'watching',\n",
       " 'eh',\n",
       " 'u',\n",
       " 'remember',\n",
       " '2',\n",
       " 'spell',\n",
       " 'name',\n",
       " 'yes',\n",
       " 'v',\n",
       " 'naughty',\n",
       " 'make',\n",
       " 'v',\n",
       " 'wet',\n",
       " 'fine',\n",
       " 'thataos',\n",
       " 'way',\n",
       " 'u',\n",
       " 'feel',\n",
       " 'thataos',\n",
       " 'way',\n",
       " 'gota',\n",
       " 'b',\n",
       " 'seriously',\n",
       " 'spell',\n",
       " 'name',\n",
       " 'ium',\n",
       " 'going',\n",
       " 'try',\n",
       " '2',\n",
       " 'month',\n",
       " 'ha',\n",
       " 'ha',\n",
       " 'joking',\n",
       " 'i_',\n",
       " 'pay',\n",
       " 'first',\n",
       " 'lar',\n",
       " 'da',\n",
       " 'stock',\n",
       " 'comin',\n",
       " 'aft',\n",
       " 'finish',\n",
       " 'lunch',\n",
       " 'go',\n",
       " 'str',\n",
       " 'lor',\n",
       " 'ard',\n",
       " '3',\n",
       " 'smth',\n",
       " 'lor',\n",
       " 'u',\n",
       " 'finish',\n",
       " 'ur',\n",
       " 'lunch',\n",
       " 'already',\n",
       " 'ffffffffff',\n",
       " 'alright',\n",
       " 'way',\n",
       " 'meet',\n",
       " 'sooner',\n",
       " 'forced',\n",
       " 'eat',\n",
       " 'slice',\n",
       " 'im',\n",
       " 'really',\n",
       " 'hungry',\n",
       " 'tho',\n",
       " 'suck',\n",
       " 'mark',\n",
       " 'getting',\n",
       " 'worried',\n",
       " 'know',\n",
       " 'im',\n",
       " 'sick',\n",
       " 'turn',\n",
       " 'pizza',\n",
       " 'lol',\n",
       " 'lol',\n",
       " 'always',\n",
       " 'convincing',\n",
       " 'catch',\n",
       " 'bus',\n",
       " 'frying',\n",
       " 'egg',\n",
       " 'make',\n",
       " 'tea',\n",
       " 'eating',\n",
       " 'mom',\n",
       " 'left',\n",
       " 'dinner',\n",
       " 'feel',\n",
       " 'love',\n",
       " 'im',\n",
       " 'back',\n",
       " 'amp',\n",
       " 'packing',\n",
       " 'car',\n",
       " 'ill',\n",
       " 'let',\n",
       " 'know',\n",
       " 'there',\n",
       " 'room',\n",
       " 'ahhh',\n",
       " 'work',\n",
       " 'vaguely',\n",
       " 'remember',\n",
       " 'feel',\n",
       " 'like',\n",
       " 'lol',\n",
       " 'wait',\n",
       " 'thats',\n",
       " 'still',\n",
       " 'clear',\n",
       " 'sure',\n",
       " 'sarcastic',\n",
       " 'thats',\n",
       " 'x',\n",
       " 'doesnt',\n",
       " 'want',\n",
       " 'live',\n",
       " 'u',\n",
       " 'yeah',\n",
       " 'got',\n",
       " '2',\n",
       " 'v',\n",
       " 'apologetic',\n",
       " 'n',\n",
       " 'fallen',\n",
       " 'actin',\n",
       " 'like',\n",
       " 'spoilt',\n",
       " 'child',\n",
       " 'got',\n",
       " 'caught',\n",
       " 'till',\n",
       " '2',\n",
       " 'wont',\n",
       " 'go',\n",
       " 'badly',\n",
       " 'cheer',\n",
       " 'k',\n",
       " 'tell',\n",
       " 'anything',\n",
       " 'fear',\n",
       " 'fainting',\n",
       " 'housework',\n",
       " 'quick',\n",
       " 'cuppa',\n",
       " 'yup',\n",
       " 'ok',\n",
       " 'go',\n",
       " 'home',\n",
       " 'look',\n",
       " 'timing',\n",
       " 'msg',\n",
       " 'i_',\n",
       " 'xuhui',\n",
       " 'going',\n",
       " 'learn',\n",
       " '2nd',\n",
       " 'may',\n",
       " 'lesson',\n",
       " '8am',\n",
       " 'oops',\n",
       " 'ill',\n",
       " 'let',\n",
       " 'know',\n",
       " 'roommate',\n",
       " 'done',\n",
       " 'see',\n",
       " 'letter',\n",
       " 'b',\n",
       " 'car',\n",
       " 'anything',\n",
       " 'lor',\n",
       " 'u',\n",
       " 'decide',\n",
       " 'hello',\n",
       " 'hows',\n",
       " 'saturday',\n",
       " 'go',\n",
       " 'texting',\n",
       " 'see',\n",
       " 'youd',\n",
       " 'decided',\n",
       " 'anything',\n",
       " 'tomo',\n",
       " 'im',\n",
       " 'trying',\n",
       " 'invite',\n",
       " 'anything',\n",
       " 'pls',\n",
       " 'go',\n",
       " 'ahead',\n",
       " 'watt',\n",
       " 'wanted',\n",
       " 'sure',\n",
       " 'great',\n",
       " 'weekend',\n",
       " 'abiola',\n",
       " 'forget',\n",
       " 'tell',\n",
       " 'want',\n",
       " 'need',\n",
       " 'crave',\n",
       " 'love',\n",
       " 'sweet',\n",
       " 'arabian',\n",
       " 'steed',\n",
       " 'mmmmmm',\n",
       " 'yummy',\n",
       " 'seeing',\n",
       " 'great',\n",
       " 'hope',\n",
       " 'like',\n",
       " 'man',\n",
       " 'well',\n",
       " 'endowed',\n",
       " 'ltgt',\n",
       " 'inch',\n",
       " 'callsmessagesmissed',\n",
       " 'call',\n",
       " 'didnt',\n",
       " 'get',\n",
       " 'hep',\n",
       " 'b',\n",
       " 'immunisation',\n",
       " 'nigeria',\n",
       " 'fair',\n",
       " 'enough',\n",
       " 'anything',\n",
       " 'going',\n",
       " 'yeah',\n",
       " 'hopefully',\n",
       " 'tyler',\n",
       " 'cant',\n",
       " 'could',\n",
       " 'maybe',\n",
       " 'ask',\n",
       " 'around',\n",
       " 'bit',\n",
       " 'u',\n",
       " 'dont',\n",
       " 'know',\n",
       " 'stubborn',\n",
       " 'didnt',\n",
       " 'even',\n",
       " 'want',\n",
       " 'go',\n",
       " 'hospital',\n",
       " 'kept',\n",
       " 'telling',\n",
       " 'mark',\n",
       " 'im',\n",
       " 'weak',\n",
       " 'sucker',\n",
       " 'hospital',\n",
       " 'weak',\n",
       " 'sucker',\n",
       " 'thinked',\n",
       " 'first',\n",
       " 'time',\n",
       " 'saw',\n",
       " 'class',\n",
       " 'gram',\n",
       " 'usually',\n",
       " 'run',\n",
       " 'like',\n",
       " 'ltgt',\n",
       " 'half',\n",
       " 'eighth',\n",
       " 'smarter',\n",
       " 'though',\n",
       " 'get',\n",
       " 'almost',\n",
       " 'whole',\n",
       " 'second',\n",
       " 'gram',\n",
       " 'ltgt',\n",
       " 'k',\n",
       " 'fyi',\n",
       " 'x',\n",
       " 'ride',\n",
       " 'early',\n",
       " 'tomorrow',\n",
       " 'morning',\n",
       " 'he',\n",
       " 'crashing',\n",
       " 'place',\n",
       " 'tonight',\n",
       " 'wow',\n",
       " 'never',\n",
       " 'realized',\n",
       " 'embarassed',\n",
       " 'accomodations',\n",
       " 'thought',\n",
       " 'liked',\n",
       " 'since',\n",
       " 'best',\n",
       " 'could',\n",
       " 'always',\n",
       " 'seemed',\n",
       " 'happy',\n",
       " 'cave',\n",
       " 'im',\n",
       " 'sorry',\n",
       " 'didnt',\n",
       " 'dont',\n",
       " 'give',\n",
       " 'im',\n",
       " 'sorry',\n",
       " 'offered',\n",
       " 'im',\n",
       " 'sorry',\n",
       " 'room',\n",
       " 'embarassing',\n",
       " 'know',\n",
       " 'mallika',\n",
       " 'sherawat',\n",
       " 'yesterday',\n",
       " 'find',\n",
       " 'lturlgt',\n",
       " 'sorry',\n",
       " 'ill',\n",
       " 'call',\n",
       " 'later',\n",
       " 'meeting',\n",
       " 'tell',\n",
       " 'reached',\n",
       " 'yesgauti',\n",
       " 'sehwag',\n",
       " 'odi',\n",
       " 'series',\n",
       " 'gonna',\n",
       " 'pick',\n",
       " '1',\n",
       " 'burger',\n",
       " 'way',\n",
       " 'home',\n",
       " 'cant',\n",
       " 'even',\n",
       " 'move',\n",
       " 'pain',\n",
       " 'killing',\n",
       " 'ha',\n",
       " 'ha',\n",
       " 'ha',\n",
       " 'good',\n",
       " 'joke',\n",
       " 'girl',\n",
       " 'situation',\n",
       " 'seeker',\n",
       " 'part',\n",
       " 'checking',\n",
       " 'iq',\n",
       " 'sorry',\n",
       " 'roommate',\n",
       " 'took',\n",
       " 'forever',\n",
       " 'ok',\n",
       " 'come',\n",
       " 'ok',\n",
       " 'lar',\n",
       " 'double',\n",
       " 'check',\n",
       " 'wif',\n",
       " 'da',\n",
       " 'hair',\n",
       " 'dresser',\n",
       " 'already',\n",
       " 'said',\n",
       " 'wun',\n",
       " 'cut',\n",
       " 'v',\n",
       " 'short',\n",
       " 'said',\n",
       " 'cut',\n",
       " 'look',\n",
       " 'nice',\n",
       " 'today',\n",
       " 'song',\n",
       " 'dedicated',\n",
       " 'day',\n",
       " 'song',\n",
       " 'u',\n",
       " 'dedicate',\n",
       " 'send',\n",
       " 'ur',\n",
       " 'valuable',\n",
       " 'frnds',\n",
       " 'first',\n",
       " 'rply',\n",
       " 'plane',\n",
       " 'give',\n",
       " 'month',\n",
       " 'end',\n",
       " 'wah',\n",
       " 'lucky',\n",
       " 'man',\n",
       " 'save',\n",
       " 'money',\n",
       " 'hee',\n",
       " 'finished',\n",
       " 'class',\n",
       " 'hi',\n",
       " 'babe',\n",
       " 'im',\n",
       " 'home',\n",
       " 'wanna',\n",
       " 'something',\n",
       " 'xx',\n",
       " 'kkwhere',\n",
       " 'youhow',\n",
       " 'performed',\n",
       " 'u',\n",
       " 'call',\n",
       " 'waiting',\n",
       " 'machan',\n",
       " 'call',\n",
       " 'free',\n",
       " 'thats',\n",
       " 'cool',\n",
       " 'gentleman',\n",
       " 'treat',\n",
       " 'dignity',\n",
       " 'respect',\n",
       " 'like',\n",
       " 'people',\n",
       " 'much',\n",
       " 'shy',\n",
       " 'pa',\n",
       " 'operate',\n",
       " 'ltgt',\n",
       " 'still',\n",
       " 'looking',\n",
       " 'job',\n",
       " 'much',\n",
       " 'ta',\n",
       " 'earn',\n",
       " 'sorry',\n",
       " 'ill',\n",
       " 'call',\n",
       " 'later',\n",
       " 'k',\n",
       " 'call',\n",
       " 'ah',\n",
       " 'ok',\n",
       " 'way',\n",
       " 'home',\n",
       " 'hi',\n",
       " 'hi',\n",
       " 'place',\n",
       " 'man',\n",
       " 'yup',\n",
       " 'next',\n",
       " 'stop',\n",
       " 'call',\n",
       " 'later',\n",
       " 'dont',\n",
       " 'network',\n",
       " 'urgnt',\n",
       " 'sm',\n",
       " 'real',\n",
       " 'u',\n",
       " 'getting',\n",
       " 'yo',\n",
       " 'need',\n",
       " '2',\n",
       " 'ticket',\n",
       " 'one',\n",
       " 'jacket',\n",
       " 'im',\n",
       " 'done',\n",
       " 'already',\n",
       " 'used',\n",
       " 'multis',\n",
       " 'yes',\n",
       " 'started',\n",
       " 'send',\n",
       " 'request',\n",
       " 'make',\n",
       " 'pain',\n",
       " 'came',\n",
       " 'back',\n",
       " 'im',\n",
       " 'back',\n",
       " 'bed',\n",
       " 'double',\n",
       " 'coin',\n",
       " 'factory',\n",
       " 'gotta',\n",
       " 'cash',\n",
       " 'nitros',\n",
       " 'im',\n",
       " 'really',\n",
       " 'still',\n",
       " 'tonight',\n",
       " 'babe',\n",
       " 'ela',\n",
       " 'kanoil',\n",
       " 'download',\n",
       " 'come',\n",
       " 'wen',\n",
       " 'ur',\n",
       " 'free',\n",
       " 'yeah',\n",
       " 'donut',\n",
       " 'stand',\n",
       " 'close',\n",
       " 'tho',\n",
       " 'youull',\n",
       " 'catch',\n",
       " 'something',\n",
       " 'sorry',\n",
       " 'pain',\n",
       " 'ok',\n",
       " 'meet',\n",
       " 'another',\n",
       " 'night',\n",
       " 'spent',\n",
       " 'late',\n",
       " 'afternoon',\n",
       " 'casualty',\n",
       " 'mean',\n",
       " 'havent',\n",
       " 'done',\n",
       " 'stuff42moro',\n",
       " 'includes',\n",
       " 'time',\n",
       " 'sheet',\n",
       " 'sorry',\n",
       " 'smile',\n",
       " 'pleasure',\n",
       " 'smile',\n",
       " 'pain',\n",
       " 'smile',\n",
       " 'trouble',\n",
       " 'pours',\n",
       " 'like',\n",
       " 'rain',\n",
       " 'smile',\n",
       " 'sum1',\n",
       " 'hurt',\n",
       " 'u',\n",
       " 'smile',\n",
       " 'becoz',\n",
       " 'someone',\n",
       " 'still',\n",
       " 'love',\n",
       " 'see',\n",
       " 'u',\n",
       " 'smiling',\n",
       " 'havent',\n",
       " 'planning',\n",
       " 'buy',\n",
       " 'later',\n",
       " 'check',\n",
       " 'already',\n",
       " 'lido',\n",
       " 'got',\n",
       " '530',\n",
       " 'show',\n",
       " 'e',\n",
       " 'afternoon',\n",
       " 'u',\n",
       " 'finish',\n",
       " 'work',\n",
       " 'already',\n",
       " 'watching',\n",
       " 'telugu',\n",
       " 'moviewat',\n",
       " 'abt',\n",
       " 'u',\n",
       " 'see',\n",
       " 'finish',\n",
       " 'load',\n",
       " 'loan',\n",
       " 'pay',\n",
       " 'hi',\n",
       " 'wk',\n",
       " 'ok',\n",
       " 'hols',\n",
       " 'yes',\n",
       " 'bit',\n",
       " 'run',\n",
       " 'forgot',\n",
       " 'hairdresser',\n",
       " 'appointment',\n",
       " 'four',\n",
       " 'need',\n",
       " 'get',\n",
       " 'home',\n",
       " 'n',\n",
       " 'shower',\n",
       " 'beforehand',\n",
       " 'cause',\n",
       " 'prob',\n",
       " 'u',\n",
       " 'ham',\n",
       " 'please',\n",
       " 'dont',\n",
       " 'text',\n",
       " 'anymore',\n",
       " 'nothing',\n",
       " 'else',\n",
       " 'say',\n",
       " 'okay',\n",
       " 'name',\n",
       " 'ur',\n",
       " 'price',\n",
       " 'long',\n",
       " 'legal',\n",
       " 'wen',\n",
       " 'pick',\n",
       " 'u',\n",
       " 'ave',\n",
       " 'x',\n",
       " 'am',\n",
       " 'xx',\n",
       " 'im',\n",
       " 'still',\n",
       " 'looking',\n",
       " 'car',\n",
       " 'buy',\n",
       " 'gone',\n",
       " '4the',\n",
       " 'driving',\n",
       " 'test',\n",
       " 'yet',\n",
       " 'per',\n",
       " 'request',\n",
       " 'melle',\n",
       " 'melle',\n",
       " 'oru',\n",
       " 'minnaminunginte',\n",
       " 'nurungu',\n",
       " 'vettam',\n",
       " 'set',\n",
       " 'callertune',\n",
       " 'caller',\n",
       " 'press',\n",
       " '9',\n",
       " 'copy',\n",
       " 'friend',\n",
       " 'callertune',\n",
       " 'wow',\n",
       " 'youre',\n",
       " 'right',\n",
       " 'didnt',\n",
       " 'mean',\n",
       " 'guess',\n",
       " 'gave',\n",
       " 'boston',\n",
       " 'men',\n",
       " 'changed',\n",
       " 'search',\n",
       " 'location',\n",
       " 'nyc',\n",
       " 'something',\n",
       " 'changed',\n",
       " 'cuz',\n",
       " 'signin',\n",
       " 'page',\n",
       " 'still',\n",
       " 'say',\n",
       " 'boston',\n",
       " 'umma',\n",
       " 'life',\n",
       " 'vava',\n",
       " 'umma',\n",
       " 'love',\n",
       " 'lot',\n",
       " 'dear',\n",
       " 'thanks',\n",
       " 'lot',\n",
       " 'wish',\n",
       " 'birthday',\n",
       " 'thanks',\n",
       " 'making',\n",
       " 'birthday',\n",
       " 'truly',\n",
       " 'memorable',\n",
       " 'aight',\n",
       " 'ill',\n",
       " 'hit',\n",
       " 'get',\n",
       " 'cash',\n",
       " 'would',\n",
       " 'ip',\n",
       " 'address',\n",
       " 'test',\n",
       " 'considering',\n",
       " 'computer',\n",
       " 'isnt',\n",
       " 'minecraft',\n",
       " 'server',\n",
       " 'know',\n",
       " 'grumpy',\n",
       " 'old',\n",
       " 'people',\n",
       " 'mom',\n",
       " 'like',\n",
       " 'better',\n",
       " 'lying',\n",
       " 'always',\n",
       " 'one',\n",
       " 'play',\n",
       " 'joke',\n",
       " 'dont',\n",
       " 'worry',\n",
       " 'guess',\n",
       " 'he',\n",
       " 'busy',\n",
       " 'plural',\n",
       " 'noun',\n",
       " 'research',\n",
       " 'going',\n",
       " 'dinnermsg',\n",
       " 'im',\n",
       " 'ok',\n",
       " 'wif',\n",
       " 'co',\n",
       " 'like',\n",
       " '2',\n",
       " 'try',\n",
       " 'new',\n",
       " 'thing',\n",
       " 'scared',\n",
       " 'u',\n",
       " 'dun',\n",
       " 'like',\n",
       " 'mah',\n",
       " 'co',\n",
       " 'u',\n",
       " 'said',\n",
       " 'loud',\n",
       " 'wa',\n",
       " 'ur',\n",
       " 'openin',\n",
       " 'sentence',\n",
       " 'formal',\n",
       " 'anyway',\n",
       " 'im',\n",
       " 'fine',\n",
       " 'juz',\n",
       " 'tt',\n",
       " 'im',\n",
       " 'eatin',\n",
       " 'much',\n",
       " 'n',\n",
       " 'puttin',\n",
       " 'weighthaha',\n",
       " 'anythin',\n",
       " 'special',\n",
       " 'happened',\n",
       " 'entered',\n",
       " 'cabin',\n",
       " 'pa',\n",
       " 'said',\n",
       " 'happy',\n",
       " 'bday',\n",
       " 'bos',\n",
       " 'felt',\n",
       " 'special',\n",
       " 'askd',\n",
       " '4',\n",
       " 'lunch',\n",
       " 'lunch',\n",
       " 'invited',\n",
       " 'apartment',\n",
       " 'went',\n",
       " 'goodo',\n",
       " 'yes',\n",
       " 'must',\n",
       " 'speak',\n",
       " 'friday',\n",
       " 'eggpotato',\n",
       " 'ratio',\n",
       " 'tortilla',\n",
       " 'needed',\n",
       " 'hmmmy',\n",
       " 'uncle',\n",
       " 'informed',\n",
       " 'he',\n",
       " 'paying',\n",
       " 'school',\n",
       " 'directly',\n",
       " 'pls',\n",
       " 'buy',\n",
       " 'food',\n",
       " 'new',\n",
       " 'address',\n",
       " 'applespairsall',\n",
       " 'malarky',\n",
       " 'going',\n",
       " 'sao',\n",
       " 'mu',\n",
       " 'today',\n",
       " 'done',\n",
       " '12',\n",
       " 'ii',\n",
       " 'predict',\n",
       " 'wat',\n",
       " 'time',\n",
       " 'i_ll',\n",
       " 'finish',\n",
       " 'buying',\n",
       " 'good',\n",
       " 'stuff',\n",
       " 'knowyetunde',\n",
       " 'hasnt',\n",
       " 'sent',\n",
       " 'money',\n",
       " 'yet',\n",
       " 'sent',\n",
       " 'text',\n",
       " 'bother',\n",
       " 'sending',\n",
       " 'dont',\n",
       " 'involve',\n",
       " 'anything',\n",
       " 'shouldnt',\n",
       " 'imposed',\n",
       " 'anything',\n",
       " 'first',\n",
       " 'place',\n",
       " 'apologise',\n",
       " 'room',\n",
       " 'hey',\n",
       " 'girl',\n",
       " 'r',\n",
       " 'u',\n",
       " 'hope',\n",
       " 'u',\n",
       " 'r',\n",
       " 'well',\n",
       " 'del',\n",
       " 'r',\n",
       " 'bak',\n",
       " 'long',\n",
       " 'time',\n",
       " 'c',\n",
       " 'give',\n",
       " 'call',\n",
       " 'sum',\n",
       " 'time',\n",
       " 'lucyxx',\n",
       " 'kkhow',\n",
       " 'much',\n",
       " 'cost',\n",
       " 'im',\n",
       " 'home',\n",
       " 'dear',\n",
       " 'call',\n",
       " 'tmorrowpls',\n",
       " 'accomodate',\n",
       " 'first',\n",
       " 'answer',\n",
       " 'question',\n",
       " 'haf',\n",
       " 'msn',\n",
       " 'yijuehotmailcom',\n",
       " 'call',\n",
       " 'meet',\n",
       " 'check',\n",
       " 'room',\n",
       " 'befor',\n",
       " 'activity',\n",
       " 'got',\n",
       " 'c',\n",
       " 'lazy',\n",
       " 'type',\n",
       " 'forgot',\n",
       " 'i_',\n",
       " 'lect',\n",
       " 'saw',\n",
       " 'pouch',\n",
       " 'like',\n",
       " 'v',\n",
       " 'nice',\n",
       " 'k',\n",
       " 'text',\n",
       " 'youre',\n",
       " 'way',\n",
       " 'sir',\n",
       " 'waiting',\n",
       " 'mail',\n",
       " 'swt',\n",
       " 'thought',\n",
       " 'nver',\n",
       " 'get',\n",
       " 'tired',\n",
       " 'little',\n",
       " 'thing',\n",
       " '4',\n",
       " 'lovable',\n",
       " 'person',\n",
       " 'cozsomtimes',\n",
       " 'little',\n",
       " 'thing',\n",
       " 'occupy',\n",
       " 'biggest',\n",
       " ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ham_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66898260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ham_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008c97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words), len(spam_words), len(ham_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b6e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df.label == 'ham']['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af25e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788c8546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's get some sights on word frequency by taking our words back apart\n",
    "# we will split each set of words by the spaces,\n",
    "# turn that into a list, cast that list as a Series,\n",
    "# and then take the value counts of that Series\n",
    "# We will do this for each type of word present\n",
    "ham_freq = pd.Series(ham_words).value_counts()\n",
    "spam_freq = pd.Series(spam_words).value_counts()\n",
    "all_freq = pd.Series(all_words).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beb98ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f363974",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_freq.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9579c638",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Represent text as word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4362ecc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# concat all frequencies together into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae15b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets join these words together into a dataframe based \n",
    "# on my frequency series:\n",
    "word_counts = pd.concat([ham_freq, spam_freq, all_freq], axis=1\n",
    "         ).fillna(0).astype(int)\n",
    "word_counts.columns = ['ham','spam','all']\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d614feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.sample(5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f6fdc2",
   "metadata": {},
   "source": [
    "- What are the most frequently occuring words?\n",
    "- Are there any words that uniquely identify a spam or ham message? I.e. words present in one type of message but not the other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c51a1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by 'all'\n",
    "\n",
    "word_counts.sort_values('all', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430df414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by 'ham' and 'spam' columns\n",
    "word_counts.sort_values(['ham','spam', 'all'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2797d6da",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "- ham vs spam count for 20 most common words\n",
    "- ham vs spam proportion for 20 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b7a4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rc('font', size=18)\n",
    "word_counts.sort_values(\n",
    "    'all', ascending=False\n",
    ")[['ham','spam']].head(20).plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts.sort_values('all', ascending=False)\\\n",
    " .head(20)\\\n",
    " .apply(lambda row: row/row['all'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027ce37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.rc('font', size=16)\n",
    "\n",
    "(word_counts.sort_values('all', ascending=False)\n",
    " .head(20)\n",
    " .apply(lambda row: row/row['all'], axis = 1)\n",
    " .drop(columns = 'all')\n",
    " .sort_values(by = 'spam')\n",
    " .plot.barh(stacked = True, width = 1, ec = 'k', legend=False)\n",
    ")\n",
    "plt.title('% of spam vs ham for the most common 20 words')\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0ac076",
   "metadata": {},
   "source": [
    "## n-grams\n",
    "\n",
    "**bigram**: combinations of 2 words\n",
    "\n",
    "Represent text as combinations of 2 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5dd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Today is tuesday ! ! ! and the weather is nice .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88eab190",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nltk.bigrams(sentence.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214effc",
   "metadata": {},
   "source": [
    "**Be Careful!** Make sure you are making bigrams out of *words*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9707b",
   "metadata": {},
   "source": [
    "- what are the most common bigrams? spam bigrams? ham bigrams?\n",
    "- visualize 20 most common bigrams, most common ham bigrams\n",
    "- ngrams\n",
    "\n",
    "Find the most common bigram and then find a representative text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3067b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nltk.ngrams(sentence.split(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90d50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(nltk.bigrams(spam_words)\n",
    "         ).value_counts().head(20).plot.barh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4a150",
   "metadata": {},
   "source": [
    "## Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934455d2",
   "metadata": {},
   "source": [
    "`python -m pip install --upgrade wordcloud`\n",
    "\n",
    "documentation: https://amueller.github.io/word_cloud/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1937c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4908a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ' '.join(ham_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33842797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a word cloud:\n",
    "# call WordCloud.generate \n",
    "# reference a string with all words, not a\n",
    "# list or series\n",
    "# call plt.imshow on the generation from wordcloud\n",
    "img = WordCloud(background_color='white'\n",
    "               ).generate(' '.join(ham_words))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Most Common Ham Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2985447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a word cloud:\n",
    "# call WordCloud.generate \n",
    "# reference a string with all words, not a\n",
    "# list or series\n",
    "# call plt.imshow on the generation from wordcloud\n",
    "img = WordCloud(background_color='white'\n",
    "               ).generate(' '.join(spam_words))\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.title('Most Common Spam Words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aeed693",
   "metadata": {},
   "source": [
    "## Other Common Features\n",
    "\n",
    "Any NLP dataset will have domain specific features, for example: number of retweets, number of @mentions, number of upvotes, or mean time to respond to a support chat. In addition to these domain specific features, some common measures for a document are:\n",
    "\n",
    "- character count\n",
    "- word count\n",
    "- sentence count\n",
    "- stopword count\n",
    "- unique word count\n",
    "- punctuation count\n",
    "- average word length\n",
    "- average words per sentence\n",
    "- word to stopword ratio\n",
    "\n",
    "Create one or more of the above features and visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ebf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add two new columns 'message_length' \n",
    "# and 'word_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6994ae1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['message_length'] = df['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba17f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider running a t_test_ind\n",
    "# spam vs ham message length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply our clean function, apply len chained on it\n",
    "# if we did not want to clean this before\n",
    "# word count, we would want to do a split on it\n",
    "df['word_count'] = \\\n",
    "df.text.apply(clean).apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7cda46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b571d7fa",
   "metadata": {},
   "source": [
    "## Sentiment\n",
    "\n",
    "A number indicating whether the document is positive or negative.\n",
    "\n",
    "- knowledge-based + statistical approach\n",
    "- relies on human-labelled data\n",
    "    - combination of qualitative and quantitative methods\n",
    "    - then empirically validate\n",
    "- different models for diff domains (e.g. social media vs news)\n",
    "- for social media\n",
    "    - Afinn ([github](https://github.com/fnielsen/afinn) + [whitepaper](http://www2.imm.dtu.dk/pubdb/edoc/imm6006.pdf))\n",
    "    - Vader ([github](https://github.com/cjhutto/vaderSentiment) + [whitepaper](http://comp.social.gatech.edu/papers/icwsm14.vader.hutto.pdf)) `nltk.sentiment.vader.SentimentIntensityAnalyzer`. Pre-trained sentiment analyzer (**V**alence **A**ware **D**ictionary and s**E**ntiment **R**easoner).)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0743b28",
   "metadata": {},
   "source": [
    "From your terminal:\n",
    "`python -c 'import nltk;nltk.download(\"vader_lexicon\")'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0cf3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.sentiment\n",
    "# we call nltk.sentiment.SentimentIntensityAnalyser()\n",
    "# use polarity_scores from that object\n",
    "sia = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores('He is really good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319cbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('He is really good!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb7af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('He is REALLY good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cb55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('He is very good!!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores(\n",
    "    'The food is good but service is slow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('He is good :-)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b0204",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('she is vegan :/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb65a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('I hate you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a492ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('I hate you :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faf4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sia.polarity_scores('I hate hate speech')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac0956b",
   "metadata": {},
   "source": [
    "Things that can influence Sentiment Score:\n",
    "1. Punctuations. Can increase the intensity\n",
    "2. Capitalization. Can increase the intensity\n",
    "3. Degree modifiers\n",
    "4. Conjunctions\n",
    "\n",
    "It can handle Emojis and slangs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc940c3",
   "metadata": {},
   "source": [
    "Apply this to the text message data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fef9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the sentiment from each of the texts as they stand:\n",
    "# apply a lambda function on each cell in the text column:\n",
    "# polarity_score's value associtated with the \"compound\"\n",
    "# key for each score\n",
    "df['sentiment'] = df['text'].apply(lambda doc: sia.polarity_scores(doc)['compound'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the mean and median values of sentiment score different for ham vs spam?\n",
    "df.groupby('label').sentiment.agg(['mean','median'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244c0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot message_length vs sentiment and hue by label\n",
    "sns.relplot(data = df, x = 'message_length', \n",
    "            y = 'sentiment', hue = 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f34f69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the distribution for sentiment different for ham vs spam\n",
    "sns.kdeplot(df[df.label == 'ham'].sentiment, label = 'ham')\n",
    "sns.kdeplot(df[df.label == 'spam'].sentiment, label = 'spam')\n",
    "plt.legend(['ham', 'spam'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c038f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate KDE plot for message_length vs sentiment score \n",
    "sns.kdeplot(\n",
    "    df[df.label == 'ham'].message_length,\n",
    "    df[df.label == 'ham'].sentiment, \n",
    "    levels = 30, shade = True );\n",
    "sns.kdeplot(\n",
    "            levels = 30, shade = True, alpha = 0.5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e014e",
   "metadata": {},
   "source": [
    "### Takeaways:\n",
    " - Spam messages seem to have roughly the same message length, where ham varies a lot.\n",
    " - Spam messages have a very positive sentiment\n",
    " - If we wanted to utilize these features for modeling, we would want to proceed forward with means testing to establish their viability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b659e3c",
   "metadata": {},
   "source": [
    "## More Resources\n",
    "\n",
    "- [kaggle wikipedia movie plots](https://www.kaggle.com/jrobischon/wikipedia-movie-plots)\n",
    "    - Suggestion: narrow to top n genres that aren't unknown\n",
    "- [wikitable extractor](https://wikitable2csv.ggor.de/) (Try with, e.g. [helicopter prison escapes](https://en.wikipedia.org/wiki/List_of_helicopter_prison_escapes))\n",
    "- [Textblob library](https://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50850655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0166462b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
