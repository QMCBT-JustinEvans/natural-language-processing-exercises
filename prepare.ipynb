{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5b23412",
   "metadata": {},
   "source": [
    "# Prepare Exercises\n",
    "The end result of this exercise should be a file named ```prepare.py``` that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61d437ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unicode, regex, json for text digestion\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "# nltk: natural language toolkit -> tokenization, stopwords\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# pandas dataframe manipulation, acquire script, time formatting\n",
    "import pandas as pd\n",
    "import acquire\n",
    "from time import strftime\n",
    "\n",
    "# shh, down in front\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a894dc0a",
   "metadata": {},
   "source": [
    "## 1. Define a function named ```basic_clean```. It should take in a string and apply some basic text cleaning to it:\n",
    "* Lowercase everything\n",
    "* Normalize unicode characters\n",
    "* Replace anything that is not a letter, number, whitespace or a single quote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0061e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    Description:\n",
    "    This function takes in a string and returns the string normalized, cleaned, and lowercase.\n",
    "    '''\n",
    "    string = unicodedata.normalize('NFKD', string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "    string = re.sub(r\"[^\\w0-9'\\s]\", '', string).lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a17a0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'85i think that stuff will work 4real'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_clean('I think THAT \"S@t#uff$\" will work 4-real!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3709d1b3",
   "metadata": {},
   "source": [
    "## 2. Define a function named ```tokenize```. It should take in a string and tokenize all the words in the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7e1d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and returns it tokenized.\n",
    "    '''\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    string = tokenizer.tokenize(string, return_str = True)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3688ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'worked hated hours'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('worked hated hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1723877",
   "metadata": {},
   "source": [
    "## 3. Define a function named ```stem```. It should accept some text and return the text after applying stemming to all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "959aa8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and returns the stemmed words.\n",
    "    '''\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4898d97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work hate hour'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem('worked hated hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d68253",
   "metadata": {},
   "source": [
    "## 4. Define a function named ```lemmatize```. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c09826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string and returns a string with words lemmatized.\n",
    "    '''\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    string = ' '.join(lemmas)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "314be8d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Im not quite sure that these word are long enough to be lemmtized'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize('Im not quite sure that these words are long enough to be lemmtized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f3894",
   "metadata": {},
   "source": [
    "## 5. Define a function named ```remove_stopwords```. It should accept some text and return the text after removing all the stopwords.\n",
    "* This function should define two optional parameters, ```extra_words``` and ```exclude_words```. \n",
    "* These parameters should define any additional stop words to include, and any words that we _**don't**_ want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4edd6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(string, extra_words = [], exclude_words = []):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    stopword_list = stopwords.words('english')\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    words = string.split()\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96941ffd",
   "metadata": {},
   "source": [
    "## 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe ```news_df```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b6fb9",
   "metadata": {},
   "source": [
    "## 7. Make another dataframe for the Codeup blog posts. Name the dataframe ```codeup_df```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa4d79",
   "metadata": {},
   "source": [
    "## 8. For each dataframe, produce the following columns:\n",
    "* ```title``` to hold the title\n",
    "* ```original``` to hold the original article/post content\n",
    "* ```clean``` to hold the normalized and tokenized original with the stopwords removed.\n",
    "* ```stemmed``` to hold the stemmed version of the cleaned data.\n",
    "* ```lemmatized``` to hold the lemmatized version of the cleaned data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b8a6e9",
   "metadata": {},
   "source": [
    "## 9. Ask yourself:\n",
    "* If your corpus is 493KB, would you prefer to use stemmed or lemmatized text?\n",
    "* If your corpus is 25MB, would you prefer to use stemmed or lemmatized text?\n",
    "* If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c52551c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
